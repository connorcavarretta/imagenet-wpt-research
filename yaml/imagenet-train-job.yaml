apiVersion: batch/v1
kind: Job
metadata:
  name: imagenet-train-job
spec:
  template:
    metadata:
      labels:
        app: imagenet-train-job
    spec:
      restartPolicy: Never

      initContainers:
      - name: stage-imagenet
        image: alpine:3.19
        command: ["/bin/sh","-c"]
        args:
          - |
            set -eux
            apk add --no-cache rsync findutils coreutils
            mkdir -p /cache/train /cache/val
            RSYNC_OPTS='-rltDW --info=progress2 --no-perms --no-owner --no-group --omit-dir-times --no-inc-recursive'
            echo "Start: 16-way rsync on train/ + single rsync on val/"
            ionice -c2 -n0 nice -n -5 rsync $RSYNC_OPTS /src/imagenet/val/ /cache/val/ &
            find /src/imagenet/train -mindepth 1 -maxdepth 1 -type d -print0 \
            | xargs -0 -n 1 -P 16 sh -c 'src="$0"; ionice -c2 -n0 nice -n -5 rsync '"$RSYNC_OPTS"' "$src"/ /cache/train/"$(basename "$src")"/'
            wait
            test -d /cache/train && test -d /cache/val
            echo "Copy complete."
        resources:
          requests: { cpu: "8000m", memory: 2Gi }
          limits:   { cpu: "12000m", memory: 4Gi }
        env:
          - { name: NVIDIA_VISIBLE_DEVICES, value: "none" }
        volumeMounts:
          - { name: imagenet-pvc,   mountPath: /src/imagenet, readOnly: true }
          - { name: imagenet-cache, mountPath: /cache }

      containers:
      - name: imagenet-trainer
        image: pytorch/pytorch:2.2.0-cuda11.8-cudnn8-runtime
        workingDir: /workspace
        env:
          - { name: IMAGENET_PATH, value: /data }
          - { name: NCCL_DEBUG, value: WARN }
          - { name: PYTHONUNBUFFERED, value: "1" }
          - { name: MALLOC_ARENA_MAX, value: "2" }
          - { name: OMP_NUM_THREADS,   value: "1" }
          - { name: MKL_NUM_THREADS,   value: "1" }
        resources:
          requests:
            cpu: "4000m"
            memory: 32Gi
            nvidia.com/gpu: "3"
          limits:
            cpu: "4000m"
            memory: 32Gi
            nvidia.com/gpu: "3"
        volumeMounts:
          - { name: cav-pvc,        mountPath: /workspace }
          - { name: imagenet-cache, mountPath: /data }
          - { name: dshm,           mountPath: /dev/shm }
        command: ["/bin/bash","-lc"]
        args:
          - |
            pip install --no-cache-dir --prefer-binary -r requirements.txt
            torchrun --standalone --nproc_per_node=3 \
              train.py \
              --batch-size 64 \
              --num-workers 4 \
              --epochs 300 \
              --warmup-epochs 20

      volumes:
        - name: imagenet-pvc
          persistentVolumeClaim: { claimName: imagenet-pvc }
        - name: cav-pvc
          persistentVolumeClaim: { claimName: cav-pvc }
        - name: imagenet-cache
          emptyDir: {}
        - name: dshm
          emptyDir: { medium: Memory }

      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: nvidia.com/gpu.product
                operator: In
                values: [ "NVIDIA-L4" ]
